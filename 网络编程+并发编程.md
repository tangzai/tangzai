# sock实现通信-TCP

## 代码展示

Server端代码：

```python
import socket

# 实例化套接字
sock = socket.socket()
# 绑定一个本地地址和端口
sock.bind(('10.215.111.239', 9000))
# 开启监听
sock.listen()

# 等待一个连接并以元组的形式返回一个新的套接字
conn,addr = sock.accept()
# 以字节的形式向客户端发送IP地址
conn.send(b"This is Server")
# 接收客户端传来的消息，允许客户端最多发送1024个字节的消息
msg = conn.recv(1024)
print(msg)
# 关闭会话
conn.close()
# 关闭套接字，释放端口
socket.close()
```

Client端代码

```python
import socket

sock = socket.socket()
sock.connect(('10.215.111.239', 9000))

msg = sock.recv(1024)
print(msg)
sock.send(b"this is Client")
sock.close()
```

## 整体思路

Server：

1. 实例化套接字
2. 绑定IP和端口
3. 等待连接
4. 返回连接信息和内容并发送信息给客户端

Client：

1. 实例化套接字
2. 连接服务端
3. 接收服务端的信息并向服务端发送信息

# 案例练习-私密聊天室-TCP

## Server代码

```python
import socket

sock = socket.socket()
sock.bind(("10.215.111.239",9000))
sock.listen()

conn,addr = sock.accept()
print("客户端已连接")
while True:
    send_msg = input(">>>>")
    conn.send(send_msg.encode("utf-8"))
	# sock.recv是一个阻塞操作，如果接收不到消息会yi'zhi
    recv_msg = conn.recv(1024)
    print(recv_msg.decode("utf-8"))
    if send_msg == "exit" or recv_msg.decode("utf-8") == "exit":
        conn.close()
        sock.close()
        break
```

## Client代码

```python
import socket

sock = socket.socket()
sock.connect(("10.215.111.239", 9000))

while True:
    server_recv = sock.recv(1024)
    print(server_recv.decode('utf-8'))

    client_msg = input(">>>")
    sock.send(client_msg.encode('utf-8'))
    if client_msg=="exit" or server_recv.decode("utf-8") == "exit":
        sock.close()
        break
```

## 代码思路

+ 使用 `While`循环做到多次聊天不断
+ 使用`if`判断做到随时中断聊天

# sock通信-UDP

### 前置知识

#### UDP通信

`sock = socket.socket(type=socket.SOCK_DGRAM)`：指定type=socket.SOCK_DGRAM来将通信设置为UDP通信

#### sock.recvfrom方法

这个方法会返回两个内容：

1. 消息字符串（str）
2. 元组（IP address, Port）

### Server：

```py
import socket

sock = socket.socket(type=socket.SOCK_DGRAM)
sock.bind(("10.215.111.239", 9000))

while True:
    msg, addr = sock.recvfrom(1024)
    print(msg.decode('utf-8'))

    send_msg = input(">>>").encode('utf-8')
    sock.sendto(send_msg, addr)
    if msg.decode('utf-8') == "exit" or send_msg.decode('utf-8') == "utf-8":
        print("聊天结束")
        sock.close()
        break
```

### Client

```python
import socket

sock = socket.socket(type=socket.SOCK_DGRAM)

while True:
    msg = input(">>>>").encode('utf-8')
    sock.sendto(msg, ("10.215.111.239", 9000))

    server_msg = sock.recv(1024).decode('utf-8')
    print(server_msg)
    if msg.decode('utf-8') == 'exit' or server_msg == 'exit':
        print("聊天结束")
        sock.close()
        break
```

# 案例联系-公开聊天室-UDP

需求：实现多人聊天（群聊）

解决思路：Server端使用列表将所有的sock都存起来，使用for循环做消息转发

遗留问题：群聊功能逻辑上已经实现！

1. 由于客户端的`input`阻塞了内容的显示。导致看起来依然是一对一聊天！
2. 就算将接收和发送两个模块分开并使用`try`，`sock.recv`也会一直阻塞程序等待信息

## Server

```py
import socket

# 指定type类型为：socket.SOCK_DGRAM；UDP通信
sock = socket.socket(type=socket.SOCK_DGRAM)
sock.bind(("10.215.111.239", 9000))
user_list = []

while True:
    # recvfrom 会返回 消息字符串 还有 (IP address, Port)
    msg, addr = sock.recvfrom(1024)

    # 将连接进来的sock都存储到列表里面
    if addr not in user_list:
        user_list.append(addr)

    print("msg", msg)
    print(user_list)
    # for循环，根据列表有的用户做转发
    for i in user_list:
        sock.sendto(msg, i)
```

## Client00

```py
import socket

sock = socket.socket(type=socket.SOCK_DGRAM)

while True:
    msg = input(">>>>").encode('utf-8')
    sock.sendto(msg, ("10.215.111.239", 9000))

    server_msg = sock.recv(1024).decode('utf-8')
    print(server_msg)
```

## Client01

```py
import socket

sock = socket.socket(type=socket.SOCK_DGRAM)

while True:
    msg = input(">>>>").encode('utf-8')
    sock.sendto(msg, ("10.215.111.239", 9000))

    server_msg = sock.recv(1024).decode('utf-8')
    print(server_msg)
```

# 案例练习-认证聊天室-TCP

## 需求：

1. 联动数据库，在数据库中查询用户账户密码
2. 加密传输：使用哈希算法保证数据的安全
3. 挑战连接：在登录前，客户端向服务端发起挑战，服务器向客户端发送挑战报文（盐）

## 代码思路

1. 用户向客户端发送挑战报文，获取盐
2. 客户端向用户发送盐
3. 客户端向服务器发送两条信息：
   1. 用户名
   2. 哈希算法后的密文
4. 服务器解析数据并从数据库中查询用户的密码
5. 服务器使用用户的密码配合盐做哈希
6. 服务器比对自己得出的哈希和用户的哈希，正确则通过

## 数据库操作

```mysql
# 创建数据库
mysql> create database pyuser;
Query OK, 1 row affected (0.00 sec)

# 创建数据表
mysql> use pyuser;
Database changed
mysql> create table users(
    -> id int not null primary key,
    -> username char(10) not null,
    -> password char(10) not null
    -> );
Query OK, 0 rows affected (0.02 sec)

# 插入数据
mysql> insert into users values(null, "Alies", "Aa123456"),(null, "Bob", "Aa123456"),(null, "Tom", "Aa123456");
Query OK, 3 rows affected (0.01 sec)
Records: 3  Duplicates: 0  Warnings: 0

mysql> select * from users;
+----+----------+----------+
| id | username | password |
+----+----------+----------+
|  1 | Alies    | Aa123456 |
|  2 | Bob      | Aa123456 |
|  3 | Tom      | Aa123456 |
+----+----------+----------+
3 rows in set (0.00 sec)
```

## 源码

### 数据库处理模块

文件名：sql.py

```py
import pymysql

# 数据库初始化
db = pymysql.connect(host='127.0.0.1', user='root', password='root', port=3306, db='pyuser')
# 获取数据库的操作光标（命令执行权限）
cursor = db.cursor()


def sql_check(username):
    sql = f'select * from users where username="{username}"'
    # 执行SQL语句
    cursor.execute(sql)
    # 查询所有SQL语句得到的结果
    data = cursor.fetchall()
    # 返回用户信息
    if data:
        for info_tuples in data:
            return info_tuples


# 测试模块
if __name__ == '__main__':
    end = sql_check("Alies")
    print(end)
```

### 用户信息认证模块

文件名：check_login.py

```python
import hashlib
from sql import sql_check

status_dict = {"opt": "login", "result": 400}


def check_login(username, passwod_md5, salt):
    # 数据库交互，查询用户密码
    info_data = sql_check(username)

    # 对用户密码进行哈希判断密码是否正确
    sql_password = info_data[2]

    md5 = hashlib.md5(str(salt).encode('utf-8'))
    md5.update(sql_password.encode('utf-8'))
    if passwod_md5 == md5.hexdigest():
        status_dict['result'] = 200
    return status_dict
```

### Server 端

文件名：server.py

```python
import socket
from check_login import check_login
import json
import random

sock = socket.socket()
sock.bind(("10.215.111.239", 9000))
sock.listen(5)
conn, addr = sock.accept()

# 挑战模块
cli_msg = conn.recv(1024).decode('utf-8')
# 盐发送
salt_random = random.randint(1, 1024)
conn.send(str(salt_random).encode('utf-8'))

# 接收客户端的加密信息 - json格式
encry_data = conn.recv(1024).decode("utf-8")
# 将json数据转成dict
encry_data = json.loads(encry_data)

# 用户身份验证
username = encry_data['username']
password_md5 = encry_data['password']
result = check_login(username, password_md5, salt_random)

if result["result"] == 200:
    conn.send("200".encode('utf-8'))
    while True:
        # 通信模块
        cli_msg = conn.recv(1024).decode('utf-8')
        print(cli_msg)
        server_msg = input(">>>>")
        conn.send(server_msg.encode("utf-8"))
        if cli_msg == "exit" or server_msg == "exit":
            print("通信结束")
            conn.close()
            break
else:
    # 登录失败，终止程序
    conn.send("400".encode("utf-8"))
    conn.close()
```

### Client 端

server：client.py

```python
import socket
import hashlib
import json

# 账号密码输入
username = input("用户名：")
password = input("密码：")


def md5_hash(salt, passwd):
    """
    所有字符串在做哈希算法之前，都必须先做一个”utf-8“编码
    :param salt: 哈希盐
    :param passwd: 密码
    :return: 密码哈希
    """
    md5 = hashlib.md5(salt.encode("utf-8"))
    md5.update(passwd.encode("utf-8"))
    return md5.hexdigest()


sock = socket.socket()
sock.connect(("10.215.111.239", 9000))

sock.send("Challenge".encode("utf-8"))
salt_random = sock.recv(1024).decode("utf-8")
# print(salt_random)

# 用户信息打包
data_json = json.dumps({"username": username, "password": md5_hash(salt_random, password)})
# 向服务器发送加密信息
sock.send(data_json.encode("utf-8"))

# 接收结果
login_res = sock.recv(1024).decode("utf-8")
# 登录失败！终止程序
if login_res == "400":
    exit("账号或密码错误")


print("登录成功！开始通信")
while True:
    cli_msg = input(">>>>")
    sock.send(cli_msg.encode("utf-8"))

    server_msg = sock.recv(1024).decode("utf-8")
    print(server_msg)
    if cli_msg == "exit" or server_msg == "exit":
        print("通信结束")
        sock.close()
        break
```

## 加密效果检查

使用wireshark进行抓包并追踪流

+ 抓包接口：环回口
+ wireshark过滤语法：`ip.addr == 10.215.111.239 && tcp.port == 9000`

### 第一次通信结果：

<img src="%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B+%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.assets/image-20230528173535016.png" alt="image-20230528173535016" style="zoom: 50%;" />

### 第二次通信结果

<img src="%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B+%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.assets/image-20230528174103985.png" alt="image-20230528174103985" style="zoom:50%;" />

## 结论：

可以看到每一次密文都不一样。虽然挑战报文的数据没有被加密。但是哪怕hack在知道挑战报文和加密算法后也依然很难去解出用户的密码

此时的致命问题是：后续数据的通信时以明文的方式传输的。但是本次练习的重点只在于加密的认证算法。要解决这个问题。可以这么做：

1. 用户和服务器都在本地生成一对非对称密钥
2. 用户和服务器互相交换公钥
3. 用户生成一个对称密钥并使用客户端的公钥加密发送给客户端
4. 客户端使用私钥解密并使用这个对称密钥做后续通信



# TCP粘包

## TCP粘包原因

由于TCP的两个特点：无边界、流传输。导致了TCP的粘包。再加上操作系统在处理的时候，为节省带宽资源，会将==若干个小数据包合成一个大数据包发送==。所以导致了粘包

注意：粘包只会导致若干个数据包合成一个大数据包接收。但==不会丢失数据==

### TCP粘包代码

Server：

```python
import socket
import time

sock = socket.socket()
sock.bind(("10.215.111.239", 9000))
sock.listen()

conn,addr = sock.accept()
conn.send(b"123456")
# time.sleep(0.1)
conn.send(b"654321")
conn.send(b"654321")
conn.send(b"654321")
conn.send(b"654321")
```

Client：

```python
import socket

sock = socket.socket()
sock.connect(("10.215.111.239", 9000))

msg = sock.recv(1024)
print(msg)
msg = sock.recv(1024)
print(msg)
msg = sock.recv(1024)
print(msg)
msg = sock.recv(1024)
print(msg)
msg = sock.recv(1024)
print(msg)
```

Client-输出：

```python
C:\Python\python.exe D:\Python\网络编程\粘包\client.py 
b'123456'
b'654321654321654321'
b'654321'
b''
b''
```

## TCP粘包解决

### 前置知识

要解决粘包问题，需要引入一个新模块：struct

![img](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B+%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.assets/827651-20180107202942674-547846208.png)

格式1：`struct.pack(Format, int)`

+ Format：代表模式，`'i'`代表返回4个字节的长度
+ int：代表一个整数

struct会以4个字节的形式返回整数`int`的表达方式：

`byte_len = struct.pack('i', len(msg))` 输出 `b'\x06\x00\x00\x00'`

格式2：`size = struct.unpack('i', byte_len)[0]`

unpac则是对这4个字节进行解码，返回一个元组`(1000,)`

### 源码

Server端：

```py
import socket
import struct

sock = socket.socket()
sock.bind(("10.215.111.239", 9000))
sock.listen()

conn, addr = sock.accept()

msg = b"123456"
byte_len = struct.pack('i', len(msg))
# print(byte_len)
conn.send(byte_len)
conn.send(msg)

msg = b"123456"
byte_len = struct.pack('i', len(msg))
conn.send(byte_len)
conn.send(msg)

msg = b"123456"
byte_len = struct.pack('i', len(msg))
conn.send(byte_len)
conn.send(msg)

msg = b"123456"
byte_len = struct.pack('i', len(msg))
conn.send(byte_len)
conn.send(msg)

msg = b"123456"
byte_len = struct.pack('i', len(msg))
conn.send(byte_len)
conn.send(msg)
```

Client：

```python
import socket
import struct

sock = socket.socket()
sock.connect(("10.215.111.239", 9000))

byte_len = sock.recv(4)
size = struct.unpack('i', byte_len)[0]
msg = sock.recv(size)
print(msg)

byte_len = sock.recv(4)
size = struct.unpack('i', byte_len)[0]
msg = sock.recv(size)
print(msg)

byte_len = sock.recv(4)
size = struct.unpack('i', byte_len)[0]
msg = sock.recv(size)
print(msg)

byte_len = sock.recv(4)
size = struct.unpack('i', byte_len)[0]
msg = sock.recv(size)
print(msg)

byte_len = sock.recv(4)
size = struct.unpack('i', byte_len)[0]
msg = sock.recv(size)
print(msg)
```

Client 输出：

```python
C:\Python\python.exe D:\Python\网络编程\粘包\client.py 
b'123456'
b'123456'
b'123456'
b'123456'
b'123456'

Process finished with exit code 0
```

# 案例练习-文件传输

> 需求：能支持用户的上传和下载

## 代码思路：

模仿FTP服务器，使用命令来区分客户端需要上传还是下载（pull | get）

## 源码：

Server

```python
import socket
import json
import os

sock = socket.socket()
sock.bind(("10.215.111.239", 9000))
sock.listen(5)

conn, addr = sock.accept()
file_dict = {"filename": "好好.txt | 爱转角.txt"}
file_dict_json = json.dumps(file_dict)
conn.send(file_dict_json.encode('utf-8'))

# 接收模式符
mode = conn.recv(1024).decode('utf-8')
# 模式判断
if mode == "pull":
    msg = conn.recv(4096)
    with open('tmp.txt', 'wb') as f:
        f.write(msg)

elif mode == "get":
    download_fileName = conn.recv(1024).decode('utf-8')
    path = "D:\Python\网络编程\文件传输\\" + download_fileName
    if os.path.isabs(path):
        # 文件存在，发送文件给用户
        with open(path, 'rb') as f:
            msg = f.read()
            conn.send(msg)
    else:
        # 文件不存在
        conn.send("请求文件不存在~")

else:
    conn.send("错误的模式~")
```

Client：

```python
import socket
import json
import os

sock = socket.socket()
sock.connect(("10.215.111.239", 9000))

msg_dict_json = sock.recv(4096).decode('utf-8')
msg_dict = json.loads(msg_dict_json)
print("文件列表：", msg_dict['filename'])

mode = input("上传/下载(pull/get)：")
sock.send(mode.encode('utf-8'))
if mode == 'pull':
    # 上传模块
    # 指定上传文件的路径
    file_path = input("输入文件路径：")
    if os.path.isabs(file_path):
        try:
            # 文件存在，读取文件
            with open(file_path, 'rb') as f:
                sock.send(f.read())
        except FileNotFoundError as e:
            print("文件读取失败！请检查文件mi是否正确~")
    else:
        print("文件不存在！请检查文件路径是否正确~")

elif mode == 'get':
    download_fileName = input("你要下载哪个文件：")
    sock.send(download_fileName.encode('utf-8'))
    # 接收文件
    get_data = sock.recv(4096)
    # print(get_data)
    with open("client.tmp.txt", 'wb') as f:
        f.write(get_data)
else:
    print("请重新选择")
```

# 非阻塞IO模型

> 上面所写的代码实际上都是基于阻塞IO模型实现的，其中，带有阻塞功能的代码有：
>
> ​		sock.accept	|	sock.recv

## 前置知识

设置`sock.setblocking(FALSE)`，此时socket代码将不会阻塞：accept\recv 都不会阻塞程序执行

```py
import socket

sock = socket.socket()
sock.bind(("192.168.1.105", 9000))
# 设置为非阻塞IO模型
sock.setblocking(False)

conn, addr = sock.accept()
```

# socketserver 模块

> sockerserver 模块是基于 socket 模块写的代码，可实现基于TCP的并发编程，同时为多个用户提供服务

## 源码

Server：以这个为固定格式，修改

```py
import socketserver


class Myserver(socketserver.BaseRequestHandler):
    def handle(self):      # 自动触发handle方法，并且self.request == conn
        print(self.request)


server = socketserver.ThreadingTCPServer(("192.168.1.105", 9000), Myserver)
server.serve_forever()
```

Client：

```python
import socket

sock = socket.socket()
sock.connect(("192.168.1.105", 9000))

msg = sock.recv(1024).decode('utf-8')
print(msg)

sock.send("Bob is here".encode('utf-8'))
```

# 进程和线程的了解

## 进程简介：

进程是计算机中最小的资源分配单位，系统会给每一个进程分配一块内存空间和一个PID值：

+ 内存空间：用于资源隔离
+ PID：进程的唯一标识符

一个14核的CPU在不考虑轮询的机制下，同一时间能运行14个进程

## 线程简介：

1. 线程是进程中的一部分

2. 每一个进程中至少由一个线程，线程是负责执行具体代码的

3. 一个进程中运行有多个线程同时执行

   

# 多进程：Process

## Process 多进程运行初识

```py
from multiprocessing import Process
import os
from time import sleep

def func():
    print('start', os.getpid())
    sleep(1)
    print('end', os.getpid())


if __name__ == '__main__':
    p = Process(target=func)		# 创建子进程，并给子进程分配任务
    p.start()						# 子进程启动
    print('main', os.getpid())
```

==输出==

```py
C:\Python\python.exe D:\Python\网络编程\process多进程\process1.py 
main 10260
start 8408
end 8408
```

从输出结果可以看到，代码的执行是先执行'main'，再执行函数“func”，这是因为python在创建了一个子进程“p”之后并没有执行，而是在`p.start()`的时候才开始执行，并且期间并不会等待子进程的运行。而是继续执行主进程的代码，所以最后结果是先执行“main”再执行函数“func”



## Process 多进程与`__main__`的关系

### 前置知识

`__main__`：脚本在本地直接调用的时候，`__main__`的代码会一起运行，但是如果是通过导入的方式执行的时候，`__main__`中的代码就不会运行，这是为什么？

`__main__`在本地直接运行的时候，变量`__name__ = '__main__'`，通过导入运行的时候，`__name__=文件名`，所以在导入的时候。可以通过`if __name__ == '__main__':`来判断脚本是否在本地运行

### 代码块1

可以看到，这段代码是跟上面的代码是一样的，知识没有`__main__`的判断，但是就报错了，这是因为：

Windows创建子进程的方法是通过重新导入父进程的代码来实现的，在没有`__main__`的情况下，`p.start()`会继续创建新的子进程并再次导入父进程代码。此时程序就会陷入循环，进程会不停的创建，直到系统崩溃。

```python
from multiprocessing import Process
import os
from time import sleep


def func():
    print('start', os.getpid())
    sleep(1)
    print('end', os.getpid())

# Windows操作系统执行开启进程的代码，实际上新的子进程需要通过import父进程的代码来完成数据导入的工作，所以有一些内容我们只希望在父进程中完成，就会写在 __main__ 下面
p = Process(target=func)
p.start()
print('main', os.getpid())
```

报错：

```
RuntimeError: 
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
```

## 父进程与子进程的关系

首先明确一点：父进程与子进程都是进程，都会满足进程之间资源互相隔离

对于子进程的创建，会有两种情况：

+ Windows系统：子进程会`import`父进程的代码并**重新运行计算/运行**。注意使用main预防进程的死循环
+ Linux系统：子进程会直接复制父进程的代码的运行状态，继续执行，其中不用**重新计算/运行**

父进程的结束：父进程的结束需要满足以下几点：

+ 父进程的代码执行完毕
+ 子进程的代码执行完毕
+ 父进程为子进程做资源回收完毕

## `join()`方法等待子进程的运行

多进程任务中，父进程并不会等待子进程的运行，而是继续执行下一条代码。但是在实际的业务中，可能会遇到父进程需要等待所有子进程完成任务后才能继续执行的情况，此时可以使用`join()`来阻塞主进程的运行

代码思路：将所有的进程实例都存入一个列表中，遍历这个列表并调用实例的`join`方法以等待所有的进程执行完毕

```python
import random
from multiprocessing import Process
from time import sleep


def func():
    sleep(random.randint(1,2))
    print("正在吃饭")


if __name__ == '__main__':
    process_list = []
    for i in range(10):
        p = Process(target=func)
        p.start()
        # 将所有创建的子进程写入列表里面
        process_list.append(p)

       # 循环所有子进程，使用 join() 方法等待子进程
    for j in process_list:
        j.join()

    print("吃完饭了")
```

==输出==

```python
C:\Python\python.exe D:\Python\网络编程\process多进程\process1.py 
正在吃饭
正在吃饭正在吃饭
正在吃饭

正在吃饭
正在吃饭
正在吃饭
正在吃饭
正在吃饭正在吃饭

吃完饭了
```

## 守护进程

> 守护进程会随着主进程的开启而开启，随着主进程的结束而结束

```python
from time import sleep
from multiprocessing import Process

# 守护进程：守护进程的执行会随着父进程的执行结束而结束

def son1():
    while True:
        print("is alive~")
        sleep(0.5)


if __name__ == '__main__':
    p = Process(target=son1)
    # 将子进程p设置为守护进程，这条代码必须要在p.start之前
    p.daemon = True
    p.start()
    sleep(2)
```

==输出==

```python
C:\Python\python.exe D:\Python\网络编程\守护进程\守护进程1.py 
is alive~
is alive~
is alive~
is alive~
```

## Process类的其他方法

`p.is_alive()`：判断某个进程是否存活

`p.terminate()`：强制结束某个进程

```python
from time import sleep
from multiprocessing import Process


# 守护进程：守护进程的执行会随着父进程的执行结束而结束

def son1():
    while True:
        print("is alive~")
        sleep(0.5)


if __name__ == '__main__':
    p = Process(target=son1)
    # 将子进程p设置为守护进程
    p.start()   # 异步非阻塞方法
    p.is_alive()  # 判断进程是否存活
    sleep(2)
    p.terminate()  # 异步非阻塞方法，强制结束一个进程
```

## 面向对象开启多进程

使用面向对象创建多进程要注意一下几点：

1. 记得继承`Process类`的多进程方法
2. 将子进程要执行的代码写在`run函数`中，`Process`会自动执行

```python
from multiprocessing import Process
import os


class MyProcess(Process):
    def __init__(self, names, age):
        self.names = names
        self.age = age
        super().__init__()

    def run(self):
        print(self.name)
        print(f"{self.names}今年{str(self.age)}岁")


if __name__ == '__main__':
    mp = MyProcess("小明", 18)
    mp.start()
    print("main：", os.getpid())
```

## 多进程间通信

`Quete.get`和`Quete.put`之间的个数必须一一对应，否则会发生阻塞

```python
from multiprocessing import Process, Queue


def myprocess(quete):
    ext = 2 * 2
    quete.put(ext)


if __name__ == '__main__':
    q = Queue()
    p = Process(target=myprocess, args=(q,))
    p.start()
    print(q.get())
```

## 多进程间的数据共享

多进程间的数据共享是通过文件操作来实现的，在实际的开发中，应该避免出现这种操作，否则为了保证数据的安全性，则不得不用加锁的操作来保证数据安全。这样就失去了多进程的意义，程序的运行效率也会大幅度降低

```python
from multiprocessing import Process, Manager, Lock


def func(dic, lk):
    while lk:
        dic['count'] -= 1


if __name__ == '__main__':
    manager = Manager()
    dic = manager.dict({'count': 100})
    processes = []
    # 通过锁来保证数据的安全性
    lock = Lock()
    for i in range(100):
        p = Process(target=func, args=(dic,lock))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    print(dic)
```





# 锁

在并行编程中，可能会遇到多个进程同时对某个文件或数据库进程操在的情况，且在实际的业务中，由于带有许多干扰条件：网络延迟，服务器性能等。就会导致数据不同步的情况。此时我们可以使用锁来保证数据的安全性

锁的功能：在一段加了锁的代码中，同一时间只能有一个进程来执行这段代码。其他进程需要等待

## 锁的写法

推荐使用写法二

```py
from multiprocessing import Lock

# 写法一
def buy_ticket(lock):
	lock.requite()
	代码块...
	lock.release()
	
# 写法二
def buy_ticket(lock):
	with lock:
		代码块...
	

if __name__ == '__main__':
    # 实例化锁
    lock = Lock()
    p = Process(target=buy_ticket, args=(lock, )

```





```python
import time
from multiprocessing import Process, Lock
from time import sleep
import json


def buy_ticket(username, lock):
    # 给代码加锁
    with lock:
        # lock.acquire()
        sleep(0.02)
        with open('Ticket_SQL.txt') as f:
            ticket_dict = json.load(f)

        print(ticket_dict['count'])
        if ticket_dict['count'] > 0:
            sleep(0.05)
            ticket_dict['count'] -= 1
            print(f"---->{username}买到票了")

        else:
            print(f"{username}没买到票")

        with open('Ticket_SQL.txt', 'w') as f:
            f.write(json.dumps(ticket_dict))

        # 给代码解锁
        # lock.release()


if __name__ == '__main__':
    # 实例化锁
    lock = Lock()
    for i in range(10):
        p = Process(target=buy_ticket, args=(f"user{i}", lock))
        p.start()
```

# Quete 队列

>  队列原则：先进先出

## 队列的阻塞

+ 当队列已满后再往队列中添加新元素，队列阻塞

```python
from multiprocessing import Queue

# 设置队列的大小为5
q = Queue(5)
q.put(1)
q.put(2)
q.put(3)
q.put(4)
q.put(5)
print("5555555")
# 队列已满程序阻塞
q.put(6)
```

+ 当队列没有元素之后，再从队列取元素，队列阻塞

```python
from multiprocessing import Queue

# 设置队列的大小为5
q = Queue(5)
q.put(1)
q.put(2)
q.put(3)
q.put(4)
q.put(5)


print(q.get())
print(q.get())
print(q.get())
print(q.get())
print(q.get())
# 对列中已没有元素可取，队列阻塞
print(q.get())
```

# 生产者和消费者模式

生产者和消费者模式是一种多线程并发协作的模式。在这个模式中，==一部分线程被用于生产数据，另一部分线程去处理数据==，于是便有了形象的生产者与消费者。这种模式主要是将生产者与消费者解耦，通过一个容器来解决生产者和消费者的强耦合问题，生产者消费者彼此之间不直接通讯，而是通过阻塞队列来进行通讯

==在爬虫的开发中，可以让一个进程负责做爬取，另一个进程负责做数据的处理和写入数据库==

## 生产者和消费者的简单代码

```python
from multiprocessing import Process, Queue
from time import sleep
import random


def producers(que, name, food):
    for i in range(10):
        que.put(food)
        print(f"{name}生产了{food}")


def customer(que, name):
    while True:
        sleep(random.randint(1, 2))
        food = que.get()
        if not food: break
        print(f"{name}消费了{food}")


if __name__ == '__main__':
    queue = Queue(3)
    produce = Process(target=producers, args=(queue, "小梅", "西瓜"))
    custom = Process(target=customer, args=(queue, "小狗"))
    produce.start()
    custom.start()

    # 生产完成，为预防消费者一直等待消费导致程序阻塞，需要做处理
    # 当生产者完成生成，就会往队列里面添加None，消费者在获得到None之后，知道已经消费完毕，推出程序
    produce.join()
    queue.put(None)

```

# 线程

> 由于python的劣势，一般来讲python做能做单进程单线程的操作或者多进程单线程的操作
>
> 进程是计算机的最小分配资源，线程是操作系统进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。

+ 线程 开销小 是进程的一部分

+ 进程 开销大 是一个资源的分配单位

  

## GIL锁原理

> 锁：GIL 全局解释器

一、原理：
全局解释器锁（Global Interpreter Lock，GIL）规定全局范围内任意时候一个进程里只能同时执行一个线程。每一个线程在执行时，都会锁住GIL，以阻止别的线程执行；执行一段时间后，会释放GIL，以允许别的线程开始利用资源，如果遇到阻塞情况，也会提前释放锁。

![4bf06449a57140359f97e6bd704a73a0](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B+%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.assets/4bf06449a57140359f97e6bd704a73a0.png)

如果你的程序是单线程，该GIL锁并不会对程序造成什么影响。但如果在计算密集型的多线程代码中，GIL就是一个性能瓶颈，使Python多线程成为伪并行多线程。

![f3b8f5e624a64d37ad23108caded6993](%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B+%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.assets/f3b8f5e624a64d37ad23108caded6993.png)

“全局”的理解：
如果你的服务器拥有八核，GIL锁也就是规定8核CPU同时仅能执行一个线程，如果线程1在CPU1中运行着，线程2想在CPU2上运行，只能继续等待线程1结束，获得GIL锁，才可以在CPU2上运行。
在稍微极端一点的情况下，比如线程1使用了while True在CPU 1 上执行，那就真是“一核有难，八核围观”了，如下图所示：

“计算密集型”理解：
在提到开发性能瓶颈的时候，我们经常把对资源的限制分为两类，一类是计算密集型（CPU-bound），一类是 I/O 密集型（I/O-bound）。
计算密集型的程序是指的是把 CPU 资源耗尽的程序，也就是说想要提高性能速度，就需要提供更多更强的 CPU，比如矩阵运算，图片处理这类程序。
I/O 密集型的程序只的是那些花费大量时间在等待 I/O 运行结束的程序，比如从用户指定的文件中读取数据，从数据库或者从网络中读取数据，I/O 密集型的程序对 CPU 的资源需求不是很高。
对于计算密集型应用，由于CPU一直处于被占用状态，GIL锁直到规定时间才会释放，然后才会切换状态，导致多线程处于绝对的劣势，此时可以采用多进程+协程。
对于IO密集型应用，多线程的应用和多进程应用区别不大。即便有GIL存在，由于IO操作会导致GIL释放，其他线程能够获得执行权限。由于多线程的通讯成本低于多进程，因此偏向使用多线程。

何时释放锁？
在python3.2前是通过计数，默认是100，在python3.2时已经不是通过计数来切换，而是时间间隔。其中的目标是更可预测的切换间隔和减少由于锁争用和后续系统调用数量造成的开销。允许线程切换的“检查间隔”的概念已经被放弃，取而代之的是以秒表示的绝对持续时间。该参数可通过sys.setswitchinterval()调节。它目前默认为5毫秒。

二、GIL存在的意义：
首先了解一下引用计数这个概念。Python 使用引用计数来进行内存管理，即垃圾回收。引用计数就是说 Python 里创建的所有对象，Python 都有一个变量（reference count）记录着当前有多少个引用指向了这个对象，当引用数变成 0 的时候，Python 就会回收这个对象所占用的内存。
Python 的引用计数需要避免资源竞争的问题，我们需要在有两个或多个线程同时增加或减少引用计数的情况下，依然保证引用计数的结果是正确的。
当有多个线程同时改一个对象的引用计数的时候，有可能导致内存泄漏（对象的引用计数永远没有归零的机会），还有可能导致对象提起释放，触发莫名奇妙的 bug，程序崩溃（一个对象存在引用的情况下引用计数变成了 0，导致此对象提前释放）。
通过对不同线程访问、修改引用计数增加锁，我们就可以保证引用计数总是被正确的修改。但是，如果我们对每一个对象或者每一组对象都增加锁，这就意味在在你的 Python 程序中有很多个锁同时存在。多个锁同时存在会有其他的风险–死锁。除此之外，性能下降也是多个锁存在的一大弊端。因为申请锁和释放锁都是一笔不小的开销。
GIL 是一个锁，或者一把锁，这把锁加载了 Python 的解释器上，它要求任何 Python 代码在执行的时候需要先申请这把锁，否则就别想执行。只有一把锁，带来的好处就是 1、不会有死锁，2、对因为引入锁而导致的性能下降影响不大，然而坏处就是 GIL 这把锁让计算密集型的代码也只能使用单线程执行。

三、规避措施：
1.更换解释器：
python解释器有很多个，包括 CPython, Jython, IronPython, PyPy，分别是用 C，Java，C#，Python 实现的。但GIL 锁只存在于 CPython 解释器中。如果你的代码和依赖库不依赖 CPython 的话，那么换其他的解释器也是可行的方案。
通常加锁也有2种不同的粒度的锁:
fine-grained(所谓的细粒度), 那么程序员需要自行地加,解锁来保证线程安全
coarse-grained(所谓的粗粒度), 那么语言层面本身维护着一个全局的锁机制,用来保证线程安全
前一种方式比较典型的是 java, Jython 等, 后一种方式比较典型的是 CPython。

2.将计算型线程改写为进程：（https://docs.python.org/zh-cn/3.9/library/multiprocessing.html）
解决 GIL 锁最通用的方法是使用多进程。因为每个 Python 进程都有自己的 Python 解释器，有自己的内存空间，有自己的 GIL 锁，相互之间没有影响。自然也就没有问题了。

3.多线程调用c动态链接库，来达到多核：
通过编写C语言扩展与Python交互，在C语言层面绕过GIL实现多核利用。使用ctypes模块。

4.协程：
协程又称用户态线程，Python3.4版本后新增了对协程的支持，也是对性能的提升提供了一种选择。

四、疑问：为什么同一个多线程程序在python2中和python3中表象不一致呢？
因为Python3 针对 GIL 做了很多优化。
上面我们说性能瓶颈的时候，谈到了代码有计算密集型和 I/O 密集型，那么如果一个程序既有计算密集型又有 I/O 密集型呢？在这种情况下，Python3.0的 GIL 知道他更应该把 GIL 分给计算密集型的线程，会饿死I/ o绑定的线程。这种机制的实现原理是强制获取到GIL 锁的线程在规定的时间段长之后交出 GIL 锁，如果此时没有其他线程申请锁，那么原来的线程可以继续持有锁，继续运行。
然而这种机制带来新的问题是大部分计算密集型线程都会在其他线程申请 GIL 锁之前再次申请 GIL 锁，这会导致有些线程永远拿不到 GIL 锁（我们虽然希望 CPU 密集型的线程能相比 I/O 密集型的线程能更长时间的获得锁，但是并不希望 CPU 密集型拿着锁永远不释放）。这个问题在 Python3.2 版本中，通过引入线程没有获取到 GIL 锁的请求次数的机制解决，并且在其他线程有机会运行前，不允许当前进程重新获得GIL。

## 线程的基本使用

多线程的使用和多进程的使用基本都一样

```python
from threading import Thread, current_thread, enumerate, active_count


def func(num):
    print(f"start a thread：{num}")


thread_list = []
for i in range(10):
    t = Thread(target=func, args=(i,))
    t.start()
    thread_list.append(t)

for j in thread_list:
    j.join()
    # print(j.ident)

print("线程执行完毕！")
# 查看当前的线程  return object
print(current_thread())
# 查看当前的线程ID return int
print(current_thread().ident)
# 查看当前正在运行的线程   return list
print(enumerate())
# 查看当前线程的总个数
print(active_count())
```

## 互斥锁和单例模式

### 对于多线程的数据处理，数据是否绝对安全？

当多个线程对同一个全局变量做`+= *= /= -= `此类操作的时候，就无法保证数据的安全性

这是因为此类算法都遵循先计算再赋值的原则，GIL锁在这个过程中会在计算时获取，再在计算完成后释放。在存储的时候重新获取，存储完成后重新释放的情况。在计算完毕释放锁和存储获取锁这段时间，其他线程可能会对其进行同样的操作，以至于==多个线程在同一时间对同一变量同时赋值==，导致数据的不安全

不管时`Threading`类中的`Lock`，还是`Processing`类中的`Lock`都属于互斥锁

### 多线程下的单例模式

为保证数据安全，单例模式下，我们需要使用互斥锁来保证数据的安全性

```python
from threading import Lock


class A:
    __instance = None
    lock = Lock()

    def __new__(cls, *args, **kwargs):
        with cls.lock:
            if cls.__instance is None:
                cls.__instance = super().__new__(cls)

        return cls.__instance

    def __init__(self, name, age):
        self.name = name
        self.age = age
```

## 死锁与互斥锁

### 什么时候会发生死锁？

在多线程任务中，每个线程的任务都需要同时获取多个资源才能完成任务：资源A，资源B。此时如果线程A拿到了资源A，但线程B拿到了资源B，就会发生死锁，因为这两个线程都会等待他所需要的资源被释放而不会主动释放他的资源

### 死锁代码

python中会产生死锁的情况：

1. 使用多个锁
2. 多个锁交替使用

```python
import threading

# 创建资源A和B
resourceA = threading.Lock()
resourceB = threading.Lock()

def thread1_func():
    with resourceA:
        print("Thread 1 acquired resource A")
        # 模拟一段耗时操作
        # 这期间，Thread 1仍然持有资源A，但尝试获取资源B
        # 如果Thread 2此时已经持有资源B，那么就会发生死锁
        import time
        time.sleep(1)
        with resourceB:
            print("Thread 1 acquired resource B")

def thread2_func():
    with resourceB:
        print("Thread 2 acquired resource B")
        # 模拟一段耗时操作
        # 这期间，Thread 2仍然持有资源B，但尝试获取资源A
        # 如果Thread 1此时已经持有资源A，那么就会发生死锁
        import time
        time.sleep(1)
        with resourceA:
            print("Thread 2 acquired resource A")

# 创建两个线程
thread1 = threading.Thread(target=thread1_func)
thread2 = threading.Thread(target=thread2_func)

# 启动线程
thread1.start()
thread2.start()

# 等待线程结束
thread1.join()
thread2.join()
```

### 递归锁

如何快速解决死锁问题？

+ 归根到底之所以尝试了死锁时因为使用了多个锁并且交替使用，如果我们此时只使用一个锁，那么所有问题就可以解决

递归锁的特点：

+ 可被多次require而不阻塞
+ require多少次就得有多少次release

#### 递归锁解决死锁代码

```python
import threading

# 创建资源A和B
# resourceA = threading.Lock()
# resourceB = threading.Lock()

# 创建递归锁
resourceA = resourceB = threading.RLock()

def thread1_func():
    with resourceA:
        print("Thread 1 acquired resource A")
        # 模拟一段耗时操作
        # 这期间，Thread 1仍然持有资源A，但尝试获取资源B
        # 如果Thread 2此时已经持有资源B，那么就会发生死锁
        import time
        time.sleep(1)
        with resourceB:
            print("Thread 1 acquired resource B")


def thread2_func():
    with resourceB:
        print("Thread 2 acquired resource B")
        # 模拟一段耗时操作
        # 这期间，Thread 2仍然持有资源B，但尝试获取资源A
        # 如果Thread 1此时已经持有资源A，那么就会发生死锁
        import time
        time.sleep(1)
        with resourceA:
            print("Thread 2 acquired resource A")


# 创建两个线程
thread1 = threading.Thread(target=thread1_func)
thread2 = threading.Thread(target=thread2_func)

# 启动线程
thread1.start()
thread2.start()

# 等待线程结束
thread1.join()
thread2.join()
```

# 队列

## 队列Queue

> 先进先出

```python
import queue

# 创建一个队列对象
q = queue.Queue()

# 向队列中添加元素
q.put(1)
q.put(2)
q.put(3)

# 获取队列中的元素
while not q.empty():
    item = q.get()
    print(item)

# 输出结果：
# 1
# 2
# 3
```

## 栈LifoQueue

> 先进后出

```python
import queue

# 创建一个LifoQueue对象
q = queue.LifoQueue()

# 向队列中添加元素
q.put(1)
q.put(2)
q.put(3)

# 获取队列中的元素
while not q.empty():
    item = q.get()
    print(item)

# 输出结果：
# 3
# 2
# 1

```

## 优先级队列

> 永远是优先级最小的先出，优先级越小越大

```python
import queue

# 创建一个PriorityQueue对象
q = queue.PriorityQueue()

# 向队列中添加元素
q.put((3, 'Apple'))
q.put((1, 'Banana'))
q.put((2, 'Orange'))

# 获取队列中的元素
while not q.empty():
    item = q.get()
    print(item[1])

# 输出结果：
# Banana
# Orange
# Apple

```

# 池

## 进程池

进程池的创建比一般的多进程更方便管理，且可以直接接收值



### 进程池案例

==为什么对于5个进程却能接收10个值？==

每个进程在执行完毕后会将得到的结果放在一块内存块，后续可以使用循环遍历这块内存块按顺序取出结果

```python
from concurrent.futures import ProcessPoolExecutor
from time import sleep
import os
import random


def func(num):
    print("start：", os.getpid())
    sleep(random.randint(1, 3))
    print("end：", os.getpid())
    # 返回值
    return f"第{num}个任务"


if __name__ == '__main__':

    # 创建5个进程
    p = ProcessPoolExecutor(5)
    ret_list = []
    for i in range(15):
        # ret接收值
        # 直接将要传入的参数写在目标函数的后面
        ret = p.submit(func, i)
        ret_list.append(ret)

    # 循环输出池
    for j in ret_list:
        print(j.result())

    # 关闭池，之后不能再提交任务
    # 阻塞程序，直到所有的进程完成任务
    p.shutdown()
    print("main：", os.getpid())
```

## 线程池

线程池和进程池的语法一样

### 线程池案例

```py
from concurrent.futures import ThreadPoolExecutor
from time import sleep
import os
import random


def func(num):
    print("start：", os.getpid())
    sleep(random.randint(1, 3))
    print("end：", os.getpid())
    # 返回值
    return f"第{num}个任务"


thread = ThreadPoolExecutor(5)
thread_list = []
for i in range(15):
    ret = thread.submit(func, i)
    thread_list.append(ret)

for j in thread_list:
    print(j.result())

thread.shutdown()
print("main：", os.getpid())
```

## 线程池回调函数

演示：add_done_callback 默认不接受传参，只能传入目标函数

```python
from concurrent.futures import ThreadPoolExecutor
# 创建一个ThreadPoolExecutor对象
executor = ThreadPoolExecutor(线程数 int)
# 提交任务并获取future对象
future = executor.submit(任务函数 function)
# 添加回调函数
future.add_done_callback(回调函数 function)
```

### 简单案例

```python
def callback(future):
    result = future.result()
    # 在这里处理结果

# 创建一个ThreadPoolExecutor对象
executor = ThreadPoolExecutor()

# 提交任务并获取future对象
future = executor.submit(some_function)

# 添加回调函数
future.add_done_callback(callback)
```

### 案例：使用闭包函数达到多参数传参

基本演示：

```python
from concurrent.futures import ThreadPoolExecutor

def callback(arg1, arg2):
    def inner_callback(future):
        result = future.result()
        print("Task completed with result:", result)
        print("Additional arguments:", arg1, arg2)
    
    return inner_callback

# 创建线程池
with ThreadPoolExecutor() as executor:
    # 提交任务并获取Future对象
    future = executor.submit(some_function, arg1, arg2)

    # 添加回调函数到Future对象上，并传递额外的参数
    future.add_done_callback(callback(arg1, arg2))

# 当任务完成时，回调函数将被调用，并且可以获取任务的结果和额外的参数
```

实战演示：

```python
import requests
import urllib3
from lxml import etree
from concurrent.futures import ThreadPoolExecutor
import pymysql
import time


def spider(url):
    detail_dict = {"title": None, "Release time": None, "Release site": None, "synopsis": None, "score": None}
    try:
        # 网页爬取
        data = requests.get(url, verify=False).text
        # 数据处理
        data_xpath = etree.HTML(data)
        title = data_xpath.xpath('//h2[@class="m-b-sm"]/text()')[0]
        release_time = data_xpath.xpath('//span[@data-v-7f856186=""]/text()')[3]
        release_site = data_xpath.xpath('//span[@data-v-7f856186=""]/text()')[0]
        synopsis = str(data_xpath.xpath('//p[@data-v-63864230=""]/text()')[0])
        synopsis = synopsis.strip()
        score = str(data_xpath.xpath('//p[@class="score m-t-md m-b-n-sm"]/text()')[0])
        score = score.strip()
        detail_dict['title'] = title
        detail_dict['Release time'] = release_time
        detail_dict['Release site'] = release_site
        detail_dict['synopsis'] = synopsis
        detail_dict['score'] = score
        # 数据返回
        return detail_dict
    except requests.exceptions.MissingSchema as e:
        print(f"爬取过程出现错误：{e}")
    except IndexError as e:
        print(f"遇到一个错误：{e}")

# 闭包函数
def callback(sql_cursor, db):
    # thread 是
    def inner_callback(thread):
        data = thread.result()
        print(f"获取数据：{data}")
        sql = f"insert into scrape_center values(null, '{data['title']}', '{data['Release time']}', '{data['Release site']}', '{data['synopsis']}', '{data['score']}')"
        try:
            sql_cursor.execute(sql)
            db.commit()
            print("输出插入成功")
        except Exception as e:
            print(f"插入数据出现错误{e}")
            db.rollback()
    # 调用内部函数
    return inner_callback


if __name__ == '__main__':
    # 数据库初始化
    sql_db = pymysql.connect(host='127.0.0.1', user='root', password='root', db='pyspider')
    cursor = sql_db.cursor()
    # 爬虫程序初始化
    urllib3.disable_warnings()
    base_url = "https://ssr1.scrape.center/detail/"

    # 线程程序初始化
    thread = ThreadPoolExecutor(10)

    for page in range(1, 82):
        page_url = base_url + str(page)
        ret = thread.submit(spider, page_url)
        # add_done_callback 本身只能传入一个函数，为了能传入多个参数，所以这里使用了闭包
        ret.add_done_callback(callback(cursor, sql_db))
```

# 协程

## 协程的概念

> **协程不是计算机提供的，是程序员人为创造的**
>
> 协程主要功能是可以控制程序在代码块中来回切换，可以理解为类似于CPU的轮询

官方介绍：

```
协程（coroutine）是一种用户态的轻量级线程，它们之间的切换不需要操作系统的干预，所以切换开销较小。协程可以在单个线程中并发执行，它们之间通过协作来共享CPU资源。

协程的一个主要优势就是能够有效地利用CPU资源，避免浪费。当一个协程遇到IO操作时，它会让出CPU给其他协程使用，而不是像线程那样阻塞等待IO操作完成。这样，CPU就能够在多个协程之间快速切换，保持高效运行，避免空闲浪费。

因此，协程在处理大量IO密集型任务时非常有用，能够提高程序的性能和响应速度。
```

线程是操作系统的最小单元，所以对于协程，操作系统是无感知的，正是如此，协程的切换基本没有开销。

协程和线程的切换：

+ 线程的切换是受限于操作系统的调度，当操作系统发现了IO操作之后，就会自动切换线程
+ 协程的切换时用户级别，是由程序员来觉得何时进行协程的切换

协程的缺点：由于协程的切换时由用户决定的，但是用户对IO操作的感知较低，所以就会造成了一直情况：程序已经出现了IO操作（阻塞）但是协程并没有做切换反而是一直等待，导致资源浪费

## 协程：gevent模块

### 基本演示：

```python
import gevent
import time
from gevent import monkey

# 将所有会造成阻塞或者IO操作的命令都重写入gevent
monkey.patch_all()


def eat():
    print("start eat")
    time.sleep(1)
    print("end eat")
    return "eat***"


def sleep():
    print("start sleep")
    time.sleep(0.5)
    print("end sleep")
    return "sleep666"


# spawn 创建协程任务
g1 = gevent.spawn(eat)
g2 = gevent.spawn(sleep)
# 阻塞程序，等待任务完成
gevent.joinall([g1, g2])
# 获取任务的返回值。必须要在任务执行完毕之后获取，否则值为None
print(g1.value)
print(g2.value)
```

# 案例一：gevent多协程实现爬虫

```python
from gevent import monkey

monkey.patch_all()
import requests
import urllib3
from lxml import etree
import gevent
import time


def spider(url):
    try:
        # 网页爬取
        data = requests.get(url, verify=False).text
        # 数据处理
        data_xpath = etree.HTML(data)
        return data_xpath
    except requests.exceptions.MissingSchema as e:
        print(f"爬取过程出现错误：{e}")


def data_exec(data_xpath):
    detail_dict = {"title": None, "Release time": None, "Release site": None, "synopsis": None, "score": None}
    try:
        title = data_xpath.xpath('//h2[@class="m-b-sm"]/text()')[0]
        release_time = data_xpath.xpath('//span[@data-v-7f856186=""]/text()')[3]
        release_site = data_xpath.xpath('//span[@data-v-7f856186=""]/text()')[0]
        synopsis = str(data_xpath.xpath('//p[@data-v-63864230=""]/text()')[0])
        synopsis = synopsis.strip()
        score = str(data_xpath.xpath('//p[@class="score m-t-md m-b-n-sm"]/text()')[0])
        score = score.strip()
        detail_dict['title'] = title
        detail_dict['Release time'] = release_time
        detail_dict['Release site'] = release_site
        detail_dict['synopsis'] = synopsis
        detail_dict['score'] = score
        # 数据返回
        return detail_dict
    except IndexError as e:
        print(f"遇到一个错误：{e}")


if __name__ == '__main__':
    start = time.time()
    # 程序初始化
    urllib3.disable_warnings()
    base_url = "https://ssr1.scrape.center/detail/"
    # 创建任务
    spiders_list = []
    for i in range(1, 82):
        page_url = base_url + str(i)
        spiders = gevent.spawn(spider, page_url)
        spiders_list.append(spiders)

    # 如果希望追求最大的资源i利用率，那么应该在爬虫任务被阻塞之前创建新的任务A，那么协程在被阻塞之后就会自动去执行任务A的任务
    # 等待爬虫任务完成返回数据
    gevent.joinall(spiders_list)
    data_exec_func_list = []
    # 创建数据处理任务
    for j in spiders_list:
        data_exec_func = gevent.spawn(data_exec, j.value)
        data_exec_func_list.append(data_exec_func)
    
    gevent.joinall(data_exec_func_list)
    for e in data_exec_func_list:
        print(e.value)

    print("所需时间：")
    print(time.time() - start)
```

# 案例2：多线程配合生产者和消费者模式实现爬虫代码

数据库创建：

```
create database pyspider;

create table scrape_center(
id int not null primary key auto_increment,
title char(80) not null,
relese_time char(50) not null,
relese_site char(50) not null,
synopsis text not null,
score text not null
);
```

爬虫源码：

```python
import queue
import requests
import urllib3
from lxml import etree
from threading import Thread
import pymysql

urllib3.disable_warnings()
base_url = "https://ssr1.scrape.center/detail/"


def spider(s_que, d_que):
    while True:
    	# 当队列为空时，退出爬虫
        if s_que.empty(): break
        url = s_que.get()
        # print(url)
        # if not url:break
        detail_dict = {"title": None, "Release time": None, "Release site": None, "synopsis": None, "score": None}
        data = requests.get(url, verify=False).text
        data_xpath = etree.HTML(data)
        try:
            title = data_xpath.xpath('//h2[@class="m-b-sm"]/text()')[0]
            release_time = data_xpath.xpath('//span[@data-v-7f856186=""]/text()')[3]
            release_site = data_xpath.xpath('//span[@data-v-7f856186=""]/text()')[0]
            synopsis = str(data_xpath.xpath('//p[@data-v-63864230=""]/text()')[0])
            synopsis = synopsis.strip()
            score = str(data_xpath.xpath('//p[@class="score m-t-md m-b-n-sm"]/text()')[0])
            score = score.strip()
            detail_dict['title'] = title
            detail_dict['Release time'] = release_time
            detail_dict['Release site'] = release_site
            detail_dict['synopsis'] = synopsis
            detail_dict['score'] = score
            d_que.put(detail_dict)
        except IndexError as e:
            print(f"遇到一个错误：{e}")


def data_exec(s_que, sql_cursor, db):
    # sleep(2)
    while True:
        data = s_que.get()
        # 当获取到None值时，退出函数
        if data is None: break
        print(f"获取数据：{data}")
        sql = f"insert into scrape_center values(null, '{data['title']}', '{data['Release time']}', '{data['Release site']}', '{data['synopsis']}', '{data['score']}')"
        try:
            sql_cursor.execute(sql)
            db.commit()
            print("输出插入成功")
        except Exception as e:
            print(f"插入数据出现错误{e}")
            db.rollback()


if __name__ == '__main__':
    # 数据库连接
    sql_db = pymysql.connect(host='127.0.0.1', user='root', password='root', db='pyspider')
    cursor = sql_db.cursor()

    # 创建队列
    url_queue = queue.Queue()
    detail_dict_queue = queue.Queue()
    # 线程列表存储
    spider_thread_list = []

    for page in range(1, 82):
        detail_url = base_url + str(page)
        url_queue.put(detail_url)
    url_queue.put(None)

    # 创建多线程做数据爬取
    for i in range(5):
        spider_thread = Thread(target=spider, args=(url_queue, detail_dict_queue))
        spider_thread.start()
        spider_thread_list.append(spider_thread)

    # 创建多线程做数据库交互
    sql_thread = Thread(target=data_exec, args=(detail_dict_queue, cursor, sql_db))
    sql_thread.start()

    # 等待爬虫线程执行完毕
    for i in spider_thread_list:
        i.join()
    # 所有爬虫执行完毕之后，往 detail_dict_queue 添加None
    detail_dict_queue.put(None)

    # 等待数据库写入线程执行完毕
    sql_thread.join()

```



# 案例总结

首先：

+ 案例1使用的是多协程
+ 案例2使用的是多线程

经过测试，案例1是比案例2速度要更快，但案例1的多协程代码并没有将协程的优势发挥出来，之所以更快的原因仅仅因为协程的**高并发**和**低开销**

要完美的利用多协程的优势，则必须要在爬虫任务被阻塞之前`gevent.joinall(spiders_list)`发布新的任务来让协程被阻塞之后自动切换执行新任务！

# 进程与线程总结：

对于并发编程或者并行编程来说，我们所追求的应该是**最小的资源最高的效率**

多进程编程特定：

1. 开销大
2. 数据隔离

多线程编程特点：

1. 开销小
2. 数据共享

==切记：多进程编程可以实现并行编程。但是多线程编程只能做多并发编程==



# python async异步编程

## 1. 协程

**协程不是计算机提供的，是程序员人为创造的**

**个人理解：在不使用多线程的情况下以更小的代价达到多线程的效果**

协程主要功能是可以控制程序在代码块中来回切换，可以理解为类似于CPU的轮询

### 1.1 greenlet 实现协程

```py
from greenlet import greenlet


def func1():
    print(1)
    g2.switch()     # 切换执行fun2函数
    print(3)
    g2.switch()


def func2():
    print(2)
    g1.switch()     # 切换执行fun1函数
    print(4)


if __name__ == '__main__':
    g1 = greenlet(func1)
    g2 = greenlet(func2)

    # 第一步：去执行fun1函数
    g1.switch()
    
    
# 输出
1
2
3
4
```

### 1.2 yield 关键字实现协程

带有`yield`关键字的函数都是生成器函数，可以直接使用for循环去遍历获取值

```py
def fun1():
    yield 1
    yield from fun2()		# yield 函数跳转去执行 fun2 函数
    yield 4


def fun2():
    yield 2
    yield 3

if __name__ == '__main__':
    fun = fun1()	
    for i in fun:
        print(i)
        
        
# 输出：
1
2
3
4
```

### 1.3 asyncio

在Python3.4及之后又的版本

**注意：在遇到IO阻塞自动切换**

```py
import asyncio


@asyncio.coroutine
def fun1():
    print(1)
    yield from asyncio.sleep(1)
    print(4)


@asyncio.coroutine
def fun2():
    print(2)
    print(3)
    yield from asyncio.sleep(1)


if __name__ == '__main__':
    # 创建任务列表
    tasks = [
        asyncio.ensure_future(fun1()),
        asyncio.ensure_future(fun2()),
    ]

    # 创建事件循环
    loop = asyncio.get_event_loop()
    # 运行事件循环
    loop.run_until_complete(asyncio.wait(tasks))

```

### 1.4 async & await

这段代码跟上面的代码是一样的，只是写法更简便

```py
import asyncio


async def fun1():
    print(1)
    await asyncio.sleep(1)
    print(4)


async def fun2():
    print(2)
    print(3)
    await asyncio.sleep(1)


if __name__ == '__main__':
    # 创建任务列表
    tasks = [
        asyncio.ensure_future(fun1()),
        asyncio.ensure_future(fun2()),
    ]

    # 创建事件循环
    loop = asyncio.get_event_loop()
    # 为事件循环分配任务比执行
    loop.run_until_complete(asyncio.wait(tasks))

```

## 2. 异步编程

### 2.1 事件循环

理解成为一个死循环，去检测并执行某些代码

```
# 伪代码

任务列表 = [任务1, 任务2, 任务3, ...]

while True:
	可执行的任务列表，已完成的任务列表 = 去任务列表中检查所有任务，将“可执行”和“已完成”的任务返回
	
	for 就绪任务 in 可执行的任务列表：
		执行已就绪的任务
		
	for 已完成的任务 in 已完成的任务列表：
		在任务列表中移除 已完成的任务
		
	如果 任务列表 中的任务都已完成，则终止循环
```

```py
import asyncio

# 创建事件循环
loop = asyncio.get_event_loop()
# 为事件循环分配任务比执行
loop.run_until_complete(asyncio.wait(tasks))
```

### 2.2 快速上手

协程函数：定义函数时候以 `async def 函数名`开头的

协程对象：执行 协程函数() 得到的协程对象

```py
async def func():
    pass

result = func()
```

**注意：执行协程函数创建对象、函数内部代码不会执行**

如果想要运行协程函数内部代码、必须要将协程对象交给事件循环来处理

```py
import asyncio


async def fun():
    print(1)


result = fun()
# loop = asyncio.get_event_loop()
# loop.run_until_complete(asyncio.wait(tasks))
asyncio.run(result)		# python 3.7 以上的写法，等同于上面两句
```

### 2.3 await

await + 可等待对象（协程对象、Futer、Task对象 -> IO等待）

示例1：

```py
import asyncio


async def fun():
    print('fun 函数开始')
    await asyncio.sleep(1)
    print('fun 函数结束')


result = fun()
asyncio.run(result)
```

示例2：

```py
import asyncio


async def fun():
    print('fun 函数开始')
    await asyncio.sleep(1)
    print('fun 函数结束')
    return '返回值'

async def start():
    print('start 函数开始')
    # 遇到IO操作挂起当前协程（任务），等IO操作完成之后再继续向下执行，当前协程挂起时，事件循环可以去执行其他协程（任务）
    res = await fun()
    print(f'start 函数结束，res值：{res}')

result = start()
asyncio.run(result)
```

示例3：

```py
import asyncio


async def fun():
    print('fun 函数开始')
    await asyncio.sleep(1)
    print('fun 函数结束')
    return '返回值'

async def start():
    print('start 函数开始')
    # 遇到IO操作挂起当前协程（任务），等IO操作完成之后再继续向下执行，当前协程挂起时，事件循环可以去执行其他协程（任务）
    res = await fun()
    print(f'start 函数结束，res值：{res}')
    res = await fun()
    print(f'start 函数结束，res值：{res}')

result = start()
asyncio.run(result)
```

**总结：`await`关键字被用于必须得到当前对象的返回值之后才能做下一步的操作的情况**

### 2.4 Task对象

**作用：在事件循环中添加多个任务**

示例1：

```py
import asyncio


async def func():
    print(1)
    await asyncio.sleep(1)
    print(2)
    return "返回值"


async def main():
    print("main 开始")

    # 创建任务并将任务添加到事件循环中
    task1 = asyncio.create_task(func())
    task2 = asyncio.create_task(func())

    print("main 结束")

    # 等待目标任务执行完毕
    ret1 = await task1
    ret2 = await task2
    print(ret1, ret2)


asyncio.run(main())

```

示例2：

```py
import asyncio


async def func():
    print(1)
    await asyncio.sleep(1)
    print(2)
    return "返回值"


async def main():
    print("main 开始")

    # 创建任务并将任务添加到事件循环中
    task_list = [
        asyncio.create_task(func()),
        asyncio.create_task(func())
    ]

    print("main 结束")

    # 等待目标任务执行完毕
    done,pending = await asyncio.wait(task_list, timeout=2)
    print(done)


asyncio.run(main())
```

示例3：

```py
import asyncio


async def func():
    print(1)
    await asyncio.sleep(1)
    print(2)
    return "返回值"


task_list = [
    asyncio.create_task(func(), name='n1'),
    asyncio.create_task(func(), name='n2')
]

done, padding = asyncio.run(asyncio.wait(task_list))

```

### 2.5 asyncio.Future 对象

Task 继承Future对象，Task对象内部await结果的处理基于Future对象来的

示例1：

```py
import asyncio


async def main():
    # 获取当前事件循环
    loop = asyncio.get_event_loop()
    # 创建一个任务（Future对象），这个任务什么都不干
    fut = loop.create_future()
    # 等待任务最终结果（Future对象），没有结果则会一直等下去
    await fut


asyncio.run(main())
```

示例2：

```py
import asyncio


async def set_after(fut):
    await asyncio.sleep(2)
    fut.set_result('666')

async def main():
    # 获取当前事件循环
    loop = asyncio.get_event_loop()
    # 创建一个任务（Future对象），没有绑定任何行为，则这个任务永远不知道什么结束
    fut = loop.create_future()
    # 创建一个任务（Task对象），绑定了set_after函数，函数内部在2s之后，会给fut赋值
    # 即手动设置future任务的最终结果，那么fut就可以介素了
    await loop.create_future(set_after(fut))
    # 等待 Future 对象获取 最红结果，否则一直等待下去
    data = await fut
    print(data)

asyncio.run(main())
```





